{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d99abd8-86e4-4307-8316-c109fae1b1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Terra Signal Hackathon\n",
    "This notebook is provided as a starting point. Feel free to use it, discard it, modify it, or pretend it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e741f18-4b60-4e13-a3d9-041257455ebb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Pandas"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c40c2a-4dc2-42db-b4b1-92190de5a3f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read file with pandas"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "file_path = \"./history.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f860a83c-7b32-4c93-9301-902275d3b455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Saving the dataset as a managed table in Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "### Let's save this dataframe into Unity Catalog\n",
    "# First, transform it into a pyspark dataframe\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Then write in overwrite mode\n",
    "spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"workspace.default.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69710b47-31a4-4e9f-9b5c-9c4235d2e776",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading the dataset back"
    }
   },
   "outputs": [],
   "source": [
    "### Reading from Unity Catalog\n",
    "# Reading it as a spark dataframe\n",
    "spark_df_read = spark.table(\"workspace.default.train\")\n",
    "\n",
    "# Transforming it into a pandas dataframe\n",
    "df_pd = spark_df_read.toPandas()\n",
    "df_pd.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b48225-ae5e-4028-9d34-a9c7b77b1e38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fake Model Example"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def prediction_function(input_df):\n",
    "    '''\n",
    "    An example model function, that just predicts randomly whether a customer will churn.\n",
    "    TODO: Make a better model.\n",
    "    '''\n",
    "    X = input_df[['customerID']].copy()\n",
    "    X['prediction'] = np.random.uniform(size=len(X)) >= 0.5\n",
    "    X['prediction'] = X['prediction'].map({True: 'Yes', False: 'No'})\n",
    "    return X\n",
    "\n",
    "test_df = pd.read_csv('inference.csv')\n",
    "prediction = prediction_function(test_df)\n",
    "print(prediction.head().transpose())\n",
    "# Use this code to save the prediction to a csv file for submission:\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "prediction.to_csv(f'prediction_<MY_GROUP_NAME>_{timestamp}.csv')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NOTEBOOK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
